{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTOqNHu6FCLQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-KEXqwMGFEmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "o2bOl6E0FEpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/test')"
      ],
      "metadata": {
        "id": "JXDud4o5FErx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer=transforms.Compose([\n",
        "    transforms.Resize((227,227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "ma5DOrBvFEt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "num_classes = 2\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "b8hJdov8FEwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_val_dataset(dataset, prvi=0.25,drugi=0.50):\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=prvi)\n",
        "   \n",
        "    datasets = {}\n",
        "    datasets['train'] = Subset(dataset, train_idx)\n",
        "    datasets['val'] = Subset(dataset, val_idx)\n",
        "\n",
        "    trainlo_idx, test_idx = train_test_split(list(range(len(datasets['val']))), test_size=drugi)\n",
        "    datasets['val'] = Subset(dataset, trainlo_idx)\n",
        "    datasets['test'] = Subset(dataset, test_idx)\n",
        "    return datasets\n",
        "\n",
        "dataset = ImageFolder('/content/test/MyDrive/LISTOVI-SVI', transform=transformer)\n",
        "print(len(dataset))\n",
        "datasets = train_val_dataset(dataset)\n",
        "print(len(datasets['train']))\n",
        "print(len(datasets['val']))\n",
        "print(len(datasets['test']))\n",
        "\n",
        "print(datasets['train'].dataset)\n",
        "\n",
        "dataloaders = {x:DataLoader(datasets[x],16, shuffle=True) for x in ['train','val','test']}\n",
        "x,y = next(iter(dataloaders['train']))\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "Y5iPa1nOFEyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(datasets['train'], batch_size=16, shuffle=True)\n",
        "train_loader\n",
        "validation_loader = DataLoader(datasets['val'], batch_size=16, shuffle=True)\n",
        "validation_loader\n",
        "test_loader = DataLoader(datasets['test'], batch_size=16, shuffle=True)\n",
        "test_loader"
      ],
      "metadata": {
        "id": "rLRbf4RUFE0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train data set:', len(train_loader))\n",
        "print('Test data set:', len(validation_loader))\n",
        "print('Test data set:', len(test_loader))"
      ],
      "metadata": {
        "id": "5iFUU4RlFE2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['diseased', 'helthy']\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "9t9KSSjfFhIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Linear(400, 120)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8pVhuu6OFhK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "model = AlexNet(num_classes=2).to(device)\n",
        "\n",
        "#Setting the loss function\n",
        "cost = nn.CrossEntropyLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#Setting the optimizer with the model parameters and learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#this is defined to print how many steps are remaining when training\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "CTGkvrG2FhNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "epochs = 10\n",
        "min_valid_loss = np.inf\n",
        "val_losses = []\n",
        "train_losses = []\n",
        "for e in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    model.train()     # Optional when not using Model Specific layer\n",
        "    for data, labels in train_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        target = model(data)\n",
        "        loss = criterion(target,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    valid_loss = 0.0\n",
        "  \n",
        "\n",
        "    model.eval()     # Optional when not using Model Specific layer\n",
        "    for data, labels in validation_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "        \n",
        "        target = model(data)\n",
        "        loss = criterion(target,labels)\n",
        "        valid_loss = loss.item() * data.size(0)\n",
        "        \n",
        " \n",
        "\n",
        "    train_losses.append((train_loss / len(train_loader)))\n",
        "    val_losses.append((valid_loss / len(validation_loader)))\n",
        "\n",
        "\n",
        "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(validation_loader)}')\n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "        # Saving State Dict\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')\n"
      ],
      "metadata": {
        "id": "_oRIgVzjFhPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(val_losses,label=\"val\")\n",
        "plt.plot(train_losses,label=\"train\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ziw-9Qf6FhRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "model = AlexNet(num_classes=3).to(device)\n",
        "\n",
        "#Setting the loss function\n",
        "cost = nn.CrossEntropyLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#Setting the optimizer with the model parameters and learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#this is defined to print how many steps are remaining when training\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "Du0ew_DPFhT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(3):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        #Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = cost(outputs, labels)\n",
        "        \t\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \t\t\n",
        "        if (i+1) % 15 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "        \t\t           .format(epoch+1, 7, i+1, total_step, loss.item()))\n",
        "                       \n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for data in tqdm(test_loader):\n",
        "  images,labels=data[0].to(device),data[1]  \n",
        "  y_true.extend(labels.numpy())\n",
        " \n",
        "  outputs=model(images)\n",
        " \n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  y_pred.extend(predicted.cpu().numpy())\n",
        "f1_score(y_true, y_pred, average='macro')\n",
        "print(f1_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\t "
      ],
      "metadata": {
        "id": "Izs08kbMFrSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "print(\"f1 score\",f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"f1 score\",f1_score(y_true, y_pred, average='micro'))\n",
        "print(\"accuracy_score\",sklearn.metrics.accuracy_score(y_true, y_pred))\n",
        "print(\"balanced accuracy_score\",sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
        "print(\"precision_score\",sklearn.metrics.precision_score(y_true, y_pred, average='macro'))\n",
        "print(\"rECALL micro\",sklearn.metrics.recall_score(y_true, y_pred, average='micro'))\n",
        "print(\"rECALL macro\",sklearn.metrics.recall_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "id": "WdcVJaCVFtEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        " \n",
        "for data in tqdm(test_loader):\n",
        "  images,labels=data[0].to(device),data[1]  \n",
        "  y_true.extend(labels.numpy())\n",
        " \n",
        "  outputs=model(images)\n",
        " \n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "class_names = ('angular_leaf_spot', 'bean_rust', 'healthy')\n",
        "dataframe = pd.DataFrame(cf_matrix, index=class_names, columns=class_names)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        " \n",
        "# Create heatmap\n",
        "sns.heatmap(dataframe, annot=True, cbar=None,cmap=\"YlGnBu\",fmt=\"d\")\n",
        " \n",
        "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
        " \n",
        "plt.ylabel(\"True Class\"), \n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "unOySFTfFuAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}